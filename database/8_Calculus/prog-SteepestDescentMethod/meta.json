{
    "filename": "prog-SteepestDescentMethod.tex",
    "title": "",
    "subtitle": "",
    "coding": "",
    "mathFields": "",
    "tags": "Linear Systems, Splitting Methods, Python",
    "relatedExercises": "",
    "solutionLength": "",
    "task": "\\section{Appetizer: Gradient, Steepest Descent and Conjugate Gradient Method}\n{\\color{navy} \\textbf{Background}\\\\\n\tLet $A \\in \\Rnn$ be symmetric and positive definite (spd). Then $A$ is in particular invertible, so that the\n\tlinear system\n\t$Ax = b$\n\thas a unique solution $x^* \\in \\Rn$  for all $b \\in \\Rn$  . Let us relate this linear system to an optimization problem.\n\tFor this purpose we define for a fixed spd matrix $A$ and fixed right-hand side $b$ the function\n\t$$f:=f_{A,b} : \\Rn  \\to \\R, x  \\to \\tfrac{1}{2} x^T Ax - b^T x.$$\n\tThen one can show the equivalence\n\t$$Ax^* = b ~~~\\iff~~~\tx^* = \\arg \\min_{x\\in\\Rn} f (x).\t$$\n\tIn words, $x^*$ solves the linear system on the left-hand side if and only if $x^*$ is the unique minimizer of the\n\tfunctional $f$. In fact, you will learn in the next semester that the condition $Ax^* = b$ is the necessary first-order optimality\n\tcondition:\n\t$$0 = \\nabla f (x) = Ax - b.$$\n\tDue to the convexity of $f$ this condition is also sufficient.\n%\tThe vector $\\nabla f (x)$ is called the gradient of $f$ at $x$ and can be considered the first-order derivative of $f$ (if $n = 1$, then this is precisely $f' (x)$ which we know from high school).\n\tConsequently, solving linear systems which involve spd matrices is equivalent to solving the associated\n\toptimization problem above, i.e., minimizing the function $f (x) = \\tfrac{1}{2} x^T Ax - b^T x$. Thus, in this context iterative methods\n\tfor linear systems, such as the Richardson iteration, can also be interpreted as optimization algorithms.\n\tLet us consider the (relaxed) Richardson iteration for $Ax = b$, i.e.,\n\t$x_{k+1} = (I - \\theta A)x_{k} + \\theta b$.\n\tAfter some minor manipulations and making use of $\\nabla f (x_{k} ) = Ax_{k} - b$ we arrive at the equivalent\n\tformulation\n    $$x_{k+1} = x_{k} - \\theta\\nabla f (x_{k} ).$$\n\tThe latter is what is called a gradient method. A step from $x_{k}$ into (an appropriately scaled) direction of the gradient\n\t$\\nabla f (x_{k} )$ yields a decrease in the objective function $f$ , i.e., $f (x_{k+1} ) \\leq  f (x_{k}$ ). Along the Richardson/\n\tGradient method the scaling (also called step size) $\\theta$ is fixed. However, one could also choose a different\n\t$\\theta_k$  in each iteration step. This gives the more general version\n\\begin{equation}\\label{steepest_descent_method}\n\tx_{k+1} = x_{k} - \\theta_k  \\nabla f (x_{k} ).\n\\end{equation}\n\tThe well known method of steepest descent is given by choosing\n\\begin{equation} \\label{steepest_descent_stepsize}\n\t\\theta_k  = \\frac{r_k^\\top r_k}{ r_k^\\top   Ar_k  },\n\\end{equation}\nwhere $r_k : = Ax_{k} - b$ is the $k$-th residual. This choice can be shown to be optimal in terms of convergence\n\tspeed. Even general, one can think of using a different preconditioner $N_k$ in each iteration step -- this will later correspond to Newton-type optimization algorithms.\n}\n~\\\\~\\\\\n\\textbf{Task}\\\\\nConsider the following setting: \n\t$$A = \t\t\t\\begin{bmatrix}\n\t2&0\\\\\n\t0&10\n\t\\end{bmatrix}, ~~b = \t\t\t\\begin{bmatrix}\n\t0\\\\\n\t0\n\t\\end{bmatrix}, ~~x_0 = \\begin{bmatrix}\n\t4\\\\\n\t1.4\n\t\\end{bmatrix}.$$\nConvince yourself that $A$ is spd. Determine the minimal and maximal eigenvalue of $A$, i.e., $\\lambda_{\\text{min}}$ and $\\lambda_{\\text{max}}$, respectively. What is the solution to $Ax=b$? Now extend your code (in particular \\verb|iter_solve()|) from previous sheets:\n\t\\begin{enumerate}\n\t\t\\item Implement the \\textbf{steepest descent method} \\eqref{steepest_descent_method} by choosing the stepsize $\\theta_k$ from \\eqref{steepest_descent_stepsize} in each iteration step.\n\t\t\\item Find a way to apply the \\href{https://en.wikipedia.org/wiki/Conjugate_gradient_method#Example_code_in_MATLAB_/_GNU_Octave}{\\textbf{conjugate gradient method}} to solve a system $Ax=b$, where $A$ is spd.\\\\\n\t\t\\textit{Hint:} You can either implement it on your own  or find a SciPy routine (for the latter: you can collect all iterates $x_k$ by using the callback interface).\n\t\t\\item \\textbf{Test:} Solve the above problem with the following routines:\n\t\t\\begin{enumerate}\n\t\t\t\\item Richardson method with $\\theta = \\frac{2}{\\lambda_{\\text{max}}}$\n\t\t\t\\item Richardson method with $\\theta = 0.9 \\cdot \\frac{2}{\\lambda_{\\text{max}}}$\n\t\t\t\\item Richardson method with optimal $\\theta = \\frac{2}{\\lambda_{\\text{min}}+\\lambda_{\\text{max}}}$\n\t\t\t\\item Steepest descent method\n\t\t\t\\item conjugate gradient method\n\t\t\\end{enumerate}\n\t\tGenerate the following two plots:\n\t\t\\begin{itemize}\n\t\t\t\\item[1.] Plot the iterates $x_k$ for all the runs into the same 2d plot (use different colors).\n\t\t\t\\item[2.] Plot the function values $f(x_k) = \\frac{1}{2} x_k^TAx_k - b^Tx_k$ for each iterate and all runs into a second plot (use different colors).\n\t\t\\end{itemize}\n\t\\end{enumerate}",
    "solution": "\\lstinputlisting[numbers=none]{prog-SteepestDescentMethod_solution.py}\n",
    "id": ""
}