\textbf{\normalsize Ridge Regression and the Minimum Norm Solution}

Let $b \in \mathbb{R}^m$ and $A \in \mathbb{R}^{m \times n}$.
Assume you are given the regularized least squares problem 
$$
\min\limits_{x \in \mathbb{R}^n} \| Ax-b \|_2^2 + \tfrac{\delta}{2} \|x\|_2^2.
$$
\begin{enumerate}
	\item Which equation does the solution $x_\delta$ of the above least squares problem solve?
	\item Assume you are given the following data
	\begin{center}
		\begin{tabular}{l|cc}
			z&-1&1\\
			\hline
			y&2&-1
		\end{tabular}
	\end{center}
	which you want to explain by a model $f:\mathbb{R} \to \mathbb{R}$ of the form 
	$$f_c(z) = c_1 + c_2z + c_3z^2.$$ 
	Implement a program to solve the regularized least squares problem
	$$
	\min\limits_{c \in \mathbb{R}^3} \sum_{i=1}^2 (f_c(z_i) - y_i)^2 + \tfrac{\delta}{2} \sum_{j=1}^{3} c_j^2
	$$
 to determine appropriate coefficients $x_\delta = (c_1^\delta, c_2^\delta, c_3^\delta)$ for \textit{\underline{multiple}} $\delta \in (0,1)$. Why is regularization an appropriate approach here?
 \item Find a routine to compute the minimum norm least squares solution $x^+$ and compare it to your solutions $x_\delta$. What do you observe for small $\delta$?
	\item Plot the measurements and the fitting polynomial corresponding to $x_\delta$ and $x^+$ into one figure. What do you observe for small $\delta$?\\~\\
	\textit{Hint:} Use 
	\begin{itemize}
		\item \verb|scipy.linalg.solve| to solve a linear system and set the correct flag that informs the function about the positivity of the matrix (see documentation),
		\item \verb|numpy.linspace| to create an array of multiple $\delta \in (0,1)$,
		\item the plot routines from previous exercises if you want.
	\end{itemize}
%	

\end{enumerate} 
