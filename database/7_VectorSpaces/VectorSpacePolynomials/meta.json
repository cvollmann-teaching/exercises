{
    "filename": "VectorSpacePolynomials.tex",
    "title": "",
    "subtitle": "",
    "coding": "",
    "mathFields": "",
    "tags": "Linear Algebra",
    "relatedExercises": "",
    "solutionLength": "",
    "task": "\\section{Vector Space of Polynomials}\n{\\color{navy}\nLet $\\F$ be a field. A set $V$ together with a mapping $+$ \n(sum) and a mapping $\\cdot$ (scalar multiplication) with \n\\begin{eqnarray*} \\begin{array}{r@{~}lr@{~}l} \n\t\t+: V \\times V & \\to V & \\cdot : \\F \\times V & \\to V \\\\ \n\t\t(v,w)       & \\mapsto v+ w    & (\\lambda, v) & \\mapsto \\lambda \\cdot v  \n\\end{array} \\end{eqnarray*} \nis called \\emph{\\textbf{vector space (or linear space)} over $\\mathbb{F}$}, if the following axioms \\textbf{VR1} and \\textbf{VR2} hold: \n\n\\begin{itemize} \n\t\\item[\\textbf{VR1}] $(V,+)$ is a commutative (or abelian) group with neutral element $0$, i.e., \n\t\\begin{itemize}\n\t\t\\item[\\textbf{G1}]Associativity: $\\forall v_1, v_2,v_3\\in V:~ v_1+(v_2+v_3)=(v_1+v_2)+v_3$\n\t\t\\item[\\textbf{G2}] Neutral element: $\\forall v \\in V: ~v + 0 = v$\n\t\t\\item[\\textbf{G3}] Inverse element: $\\forall v \\in V ~\\exists_1 (-v) \\in V:~ v+(-v)=0$\n\t\t\\item[\\textbf{G4}] Commutativity: $\\forall v_1, v_2 \\in V:~ v_1 + v_2 = v_2+v_1$\n\t\\end{itemize}\n\t\\item[\\textbf{VR2}] The scalar multiplication is consistent/compatible with \n\t$(V,+)$ in the following way:  \n\t\n\tfor $\\lambda,\\mu \\in \\F$, $v,w \\in V$ it holds that  \n\t\\begin{itemize} \n\t\t\\item[(i)] $(\\lambda + \\mu)\\cdot v = \\lambda \\cdot v + \\mu \\cdot v$ \n\t\t\\item[(ii)] $\\lambda \\cdot (v+w) = \\lambda \\cdot v + \\lambda \\cdot w$ \n\t\t\\item[(iii)] $\\lambda \\cdot (\\mu \\cdot v) = (\\lambda \\cdot \\mu) \\cdot v$  \n\t\t\\item[(iv)] $1 \\cdot v = v$ \n\t\\end{itemize} \n\\end{itemize}\nFurthermore, let $v_1,\\ldots, v_n \\in V$, then with the summation and scalar multiplication we can more generally define the \\textbf{span} as\n$$\\text{span}(v_1,\\ldots, v_n)= \\{\\sum_{i=1}^n \\lambda_i v_i: \\lambda_i \\in \\mathbb{F}\\}.$$\nFurther we say that $v_1,\\ldots, v_n \\in V$ are \\textbf{linearly independent} if \n$$\\sum_{i=1}^n \\lambda_i v_i = 0 ~~~\\Rightarrow ~~~\\lambda_i = 0 ~~\\forall~i. $$\nIf $v_1,\\ldots, v_n \\in V$ are linearly independent and $\\text{span}(v_1,\\ldots, v_n) = V$, then we call $v_1,\\ldots, v_n$ a \\textbf{basis of $V$}.\\\\\nA mapping $f\\colon V_1 \\to V_2$ between two vector spaces is called \\textbf{linear}, if $$f(\\lambda \\cdot_1 v +_1 w) = \\lambda \\cdot_2 f(v) +_2 f(w)$$\nfor all $v,w\\in V$. Here $+_1,\\cdot_1$ and $+_2,\\cdot_2$ denote the summation and scalar multiplication defined on $V_1$ and $V_2$, respectively. Examples are $\\mathbb{R}^n$ and $\\mathbb{R}^{m\\times n}$ with the usual vector/matrix sum ``$+$'' and scalar multiplication ``$\\cdot$''.\n}\n~\\\\\nNow we consider another example of a vector space: Let $n \\in \\mathbb{N}$ and $P_n(\\mathbb{R})$ be the set of all polynomials of degree $\\leq n$ on $\\mathbb{R}$, i.e., the set $P_n(\\mathbb{R})$ contains all functions $p:\\mathbb{R} \\rightarrow \\mathbb{R}$ of the form\n\\begin{align*}\np(x) = \\sum_{k=0}^n \\alpha_k x^k\n\\end{align*}\nfor some $\\alpha_0, \\dots,\\alpha_n \\in \\mathbb{R}$. We define a summation and scalar multiplication:\n\\begin{align*}\n+&\\colon P_n(\\mathbb{R}) \\times P_n(\\mathbb{R}) \\to P_n(\\mathbb{R}),~(p+q)(x) := p(x) + q(x),\\\\\n\\cdot &\\colon  \\mathbb{R} \\times P_n(\\mathbb{R}) \\to P_n(\\mathbb{R}),\\quad\\quad ~~(r\\cdot p)(x) := r\\cdot p(x) .\n\\end{align*}\n\\begin{enumerate}\n\t\\item \\textbf{VR axioms:} Please show that $ P_n(\\mathbb{R})$ together with the above defined summation and scalar multiplication forms a vector space.\\\\[0.2cm]\n\t\\textit{Hint:} Check \\textbf{VR1} and \\textbf{VR2} with $V = P_n(\\mathbb{R})$.\n\t\\item Let $k<m \\in \\mathbb{N}$. Compute \n\t$$\n\t\\lim\\limits_{x\\rightarrow\\infty} \\frac{x^k}{x^m}, ~~\\text{ and }~~ \\lim\\limits_{x\\rightarrow\\infty}  \\frac{\\sum_{k=0}^{m-1}\\alpha_k x^k}{x^m}\n\t$$\n\tfor arbitrary $\\alpha_0, \\dots, \\alpha_m \\in \\mathbb{R}$.\n\t\\item \\textbf{Monomials form a basis:} Please show that the set $B := \\lbrace q_0, \\dots, q_n \\rbrace$ with\n\t\\begin{align*}\n\tq_k: \\mathbb{R} \\rightarrow \\mathbb{R}, \\hspace{.2cm} x \\mapsto x^k,\n\t\\end{align*} \n\tis a basis of $P_n(\\mathbb{R})$. What is the dimension of the vector space $P_n(\\mathbb{R})$?\n\t\n\t\\textit{Hint: } Part (ii) basically provides the proof of linear independence and the other assertion is obvious from the definition of $p$. \n\t\\item \\textbf{Derivative as linear operator:} Show that the operator $\\mathcal{D}:P_n(\\mathbb{R}) \\rightarrow P_n(\\mathbb{R}), \\hspace{.2cm} p \\mapsto p'$, \n\twhich maps a polynomial to its first derivative, is a $\\mathbb{R}$-linear function. \\\\[0.2cm]\n\t\\textit{Hint: } For $p(x) = \\sum_{k=0}^n \\alpha_k x^k$ we have $p'(x) = \\sum_{k=0}^n \\alpha_k kx^{k-1}$. \t\t\t\t\t\n\t\n\t\\item[5.] \\textbf{Matrix representation of the derivative:}  Let $\\Phi$ be the linear, invertible function which maps a polynomial to its coefficients (coordinates in the above basis), i.e.,\n\t$$\\Phi: P_n(\\mathbb{R}) \\rightarrow \\mathbb{R}^{n+1}, \\hspace{.2cm} \\sum_{k=0}^n \\alpha_k x^k \\mapsto (\\alpha_0, \\dots, \\alpha_n).$$ \n\tPlease remark shortly why $\\Phi$ is bijective.\n\tWhat is the matrix representation of the linear function\n\t$ F:\\mathbb{R}^{n+1} \\rightarrow \\mathbb{R}^{n+1}$ defined by $$F :=\\Phi \\circ \\mathcal{D} \\circ \\Phi^{-1}  $$ with respect to the standard basis $\\lbrace e_1, \\dots, e_{n+1}\\rbrace$? More precisely, derive the matrix $A \\in \\R^{(n+1)\\times (n+1)}$ defined by\n\t$$\n\tA:=\\begin{pmatrix}\n\t|& &|\\\\\n\tF(e_1)&\\dots&F(e_{n+1})\\\\\n\t|& &|\n\t\\end{pmatrix}.\n\t$$\n\t\n%\t\\textit{Note: } This is a bonus exercise. %The representation refers to the one in Thm 9.8 and the successive remark (as $\\Pn$ is a general vector space).\n\t\n\\end{enumerate}",
    "solution": "{\\color{solution}\n\\begin{enumerate}\n\t\\item \n\t\\textbf{VR1:} Show, that $(P_n(\\mathbb{R}), +)$ is an abelian group.\\\\\n\tLet $p(x) =\\sum_{k=0}^{n}\\alpha_kx^k$, $q(x)=\\sum_{k=0}^{n}\\beta_kx^k$ and $w(x)=\\sum_{k=0}^{n}\\gamma_kx^k$ be in $P_n(\\mathbb{R})$.\n\t\\begin{itemize}\n\t\t\\item [(i)] Associativity:$$((p+q)+w)(x)=\\sum [(\\alpha_k+\\beta_k)+\\gamma_k]x^k=\\sum[\\alpha_k+(\\beta_k+\\gamma_k)]x^k=(p+(q+w))(x)$$\n\t\t\\item [(ii)] Neutral Element:\\\\$0:=\\sum_{k=0}^{n}0\\cdot x^k$, then $\\forall~p\\in P_n(\\mathbb{R})$:$$(0+p)(x)=\\sum (0+\\alpha_k)x^k=p(x)$$\n\t\t\\item [(iii)] Inverse element:\\\\For $p(x)=\\sum \\alpha_kx^k$ define $-p(x):=\\sum (-\\alpha_k)x^k$,$$\\Rightarrow~~(p+(-p))(x)=0.$$\n\t\t\\item [(iv)] Commutativity:$$(p+q)(x)=\\sum\\underbrace{(\\alpha_k+\\beta_k)}_{\\textcolor{blue}{=\\beta_k+\\alpha_k}}x^k=(q+p)(x)$$\n\t\\end{itemize}\n\t\\textbf{VR2:} Consistency properties: Let $r,s\\in\\mathbb{R}$.\n\t\\begin{itemize}\n\t\t\\item [(i)] $((r+s)p)(x)=\\sum\\underbrace{(r+s)\\alpha_k}_{\\textcolor{blue}{=r\\alpha_k+s\\alpha_k}}x^k=(rp)(x)+(sp)(x)$\n\t\t\\item[] (ii)-(iv)\n\t\t$\\checkmark$\n\t\\end{itemize}\n\t\\item \n\tLet $k<m\\in\\mathbb{N}$. Then\n\t\\begin{align*}\n\t&\\frac{x^k}{x^m}=x^{k-m}=\\frac{1}{x^{m-k}}\\stackrel{x\\rightarrow +\\infty}{\\longrightarrow} 0\\\\\n\t\\Rightarrow~~\\forall \\alpha_0,\\dots,\\alpha_m:~~&\\frac{\\sum_{k=0}^{m-1}\\alpha_kx^k}{x^m}=\\sum_{k=0}^{m-1}\\alpha^k\\underbrace{\\left(\\frac{x^k}{x^m}\\right)}_{\\textcolor{blue}{\\rightarrow 0}}\\stackrel{x\\rightarrow +\\infty}{\\longrightarrow} 0.\n\t\\end{align*}\n\tIn particular we can conclude that\n\t$$\n\t~~\\forall\\alpha_0,\\dots,\\alpha_m:~~\\sum_{k=0}^{m-1}\\alpha^kx^k\\neq x^m~\n\t$$\n\tbecause otherwise limit~$\\equiv 1$.\n\t\\item \n\t\\begin{enumerate}\n\t\t\\item \n\t\tLinear independence by 2.\\\\\n\t\tAsssume $\\exists\\alpha_0,\\dots,\\alpha_n~(m:=\\text{max}\\{k:\\alpha_k\\neq 0\\})$ not all zero with $\\sum_{k=0}^{m}\\alpha_kq_k=0$\n\t\t\\begin{align*}&\\Rightarrow~~\\sum_{k=0}^{m}\\alpha_kq_k=\\sum_{k=0}^{m-1}\\alpha_kq_k+\\alpha_mq_m=0\\\\\n\t\t&\\Rightarrow~~\\sum_{k=0}^{m}\\alpha_kx^k=(-\\alpha_m)x^m\\\\\n\t\t&\\textcolor{red}{\\text{contradiction to 2.}}\n\t\t\\end{align*}\n\t\t\\item \n\t\t\\begin{align*}\n\t\t&\\text{span}\\{q_0,\\dots,q_n\\}=P_n(\\mathbb{R})~~\\text{by definition}\\\\\n\t\t\\Rightarrow~~&\\text{dim}(P_n(\\mathbb{R}))=n+1\n\t\t\\end{align*}\n\t\\end{enumerate}\n\t\\item \n\tLet $p=\\sum_{k=0}^{n}\\alpha_kq_k$ and $w =\\sum_{k=0}^{n}\\beta_kq_k$ and $\\lambda\\in\\mathbb{R}$, then:\n\t\\begin{align*}\\text{D}(\\lambda p+w)(x)&=\\left(\\sum_{k=0}^{n}(\\lambda \\alpha_k+\\beta_k)x^k\\right)'=\\sum_{k=0}^{n}(\\lambda \\alpha_k+\\beta_k)kx^{k-1}\\\\&=\\lambda\\sum_{k=0}^{n}\\alpha_kkx^{k-1}+\\sum_{k=0}^{n}\\beta_kkx^{k-1}=\\lambda \\text{D}(p)(x)+\\text{D}(q)(x)\\end{align*}\n\t\\item[5.] We have:\n\t\\begin{align*}\n\tD:~P_n(\\mathbb{R})\\rightarrow P_n(\\mathbb{R}),~p=\\sum_{k=0}^{n}\\alpha_kq_k\\mapsto p'&=\\sum_{k=0}^{n-1}\\alpha_{k+1}(k+1)q_k\\\\\n\t\\Phi:~P_n(\\mathbb{R})\\rightarrow\\mathbb{R}^{n+1},~p=\\sum_{k=0}^{n}\\alpha_kq_k\\mapsto &(\\alpha_0,\\dots,\\alpha_n)^T\\\\\n\t&\\textcolor{blue}{=(\\pi_0(p),\\dots,\\pi_n(p))}\n\t\\end{align*}\n\t$\\rightarrow$[$\\Phi$ linear since $\\pi_j$ are linear (see lecture) and bijective since $\\{q_0,\\dots,q_n\\}$ basis, $\\Phi^{-1}:\\mathbb{R}^{n+1}\\rightarrow P_n(\\mathbb{R}),~(\\alpha_0,\\dots,,\\alpha_n)^T\\mapsto p=\\sum \\alpha_kq_k$]\\\\\n\t\\\\\n\tNow consider: $F:\\mathbb{R}^{n+1}\\rightarrow\\mathbb{R}^{n+1},~F(\\alpha):=\\Phi\\circ D\\circ\\Phi^{-1}$\n\t$$\n\t\\begin{matrix}\n\t\\textcolor{red}{(\\alpha_o,\\dots,\\alpha_n)^T}&\\mathbb{R}^{n+1}&\\xrightarrow{A}&\\mathbb{R}^{n+1}&\\textcolor{red}{(\\alpha_1,2\\alpha_2,3\\alpha_3,\\dots,n\\alpha_n,0)=:\\beta}\\\\\n\t&\\textcolor{blue}{\\{e_1,\\dots,e_{n+1}\\}}& &\\textcolor{blue}{\\{e_1,\\dots,e_{n+1}\\}}& \\\\\n\t&\\downarrow \\Phi^{-1}& &\\Phi\\uparrow& \\\\\n\t\\textcolor{red}{p=\\sum_{k=0}^{n}\\alpha_kq_k}&P_n(\\mathbb{R})&\\overrightarrow{D}&P_n(\\mathbb{R})&\\textcolor{red}{p'=\\sum_{k=0}^{n}\\beta_kq_k}\\\\\n\t&\\textcolor{blue}{\\{q_1,\\dots,q_{n+1}\\}}& &\\textcolor{blue}{\\{q_1,\\dots,q_{n+1}\\}}& \n\t\\end{matrix}\n\t$$\n\t\\begin{align*}\n\tF(\\alpha)&=(\\Phi\\circ D\\circ\\Phi^{-1})(\\alpha)\\\\\n\t&=(\\Phi\\circ D)\\left(\\sum_{k=0}^{n}\\alpha_kq_k\\right)\\\\\n\t&=\\Phi\\left(\\sum_{k=0}^{n-1}\\alpha_{k+1}(k+1)q_k\\right)\\\\\n\t&=(\\alpha_1,2\\alpha_2,3\\alpha_3,\\dots,n\\alpha_n,0)^T\n\t\\end{align*}\n\tTo obtain the matrix representation we have to evaluate $F$ on the standard basis $\\{e_1,\\dots,e_{n+1}\\}$:\n\t$$\n\tA=\\begin{pmatrix}\n\t|& &|\\\\\n\tF(e_1)&\\dots&F(e_{n+1})\\\\\n\t|& &|\n\t\\end{pmatrix}\n\t=\\begin{pmatrix}\n\t0&\\textcolor{blue}{1}&0&0&\\cdots&0\\\\\n\t0&0&\\textcolor{blue}{2}&0&\\cdots&0\\\\\n\t\\vdots&\\vdots&0&\\textcolor{blue}{\\ddots}&\\ddots&\\vdots\\\\\n\t0&0&\\cdots&\\ddots&\\textcolor{blue}{n-1}&0\\\\\n\t0&0&0&\\cdots&0&\\textcolor{blue}{n}\\\\\n\t0&0&0&\\cdots&0&0\n\t\\end{pmatrix}\n\t$$\n\\end{enumerate}\n}",
    "id": ""
}