{
    "filename": "MinNormPseudoinverse.tex",
    "title": "",
    "subtitle": "",
    "coding": "",
    "mathFields": "",
    "tags": "Least Squares Problems",
    "relatedExercises": "",
    "solutionLength": "",
    "task": "\\textbf{\\normalsize Minimum Norm Least Squares with Pseudoinverse}\n\nLet $A\\in\\R^{m\\times n},~b\\in\\Rm$. Consider the SVD $A = U\\Sigma V^\\top$ and set $A^+ = V \\Sigma^+ U^\\top$, where $\\Sigma^+ = \\text{diag}(\\frac{1}{\\sigma_1},\\ldots, \\frac{1}{\\sigma_r},0,\\ldots,0)$ is the pseudoinverse of a diagonal matrix as derived in the lecture.\nShow that $$x^+:=A^+b = \\arg \\min_{x\\in \\{x:~A^\\top Ax=A^\\top b\\}} \\|x\\|_2^2,$$\ni.e., $x^+$ is the minimum norm least squares solution.\\\\\n\\textit{Hint:} First consider the simple case that $A$ is diagonal and then use the SVD for the general case.\n",
    "solution": "{\\color{solution}\n\t\\textbf{(1) Special Case: Diagonal matrix}\\\\~\\\\\n\tLet us start with the simple case: $A\\in \\Rmn $ diagonal\n\t\\begin{align*}\n\tA&=\\begin{pmatrix}\n\ta_{11}&~&~&~&~&0\\\\\n\t~&\\ddots&~&~&~&~\\\\\n\t~&~&a_{rr}&~&~&~\\\\\n\t~&~&~&0&~&~\\\\\n\t~&~&~&~&\\ddots&~\\\\\n\t0&~&~&~&~&0\n\t\\end{pmatrix}\\in\\mathbb{R}^{m\\times n},~a_{ii}\\neq 0,~~~A^TA=\\begin{pmatrix}\n\ta_{11}^2&~&~&~&~&0\\\\\n\t~&\\ddots&~&~&~&~\\\\\n\t~&~&a_{rr}^2&~&~&~\\\\\n\t~&~&~&0&~&~\\\\\n\t~&~&~&~&\\ddots&~\\\\\n\t0&~&~&~&~&0\n\t\\end{pmatrix}\\in\\mathbb{R}^{n\\times n}\n\t\\end{align*}\n\t\\small\n\t\\underline{Normal equation:}\n\t\\begin{align*}\n\tA^TAx=A^Tb=\\begin{pmatrix}\n\ta_{11}b_1\\\\\\vdots\\\\a_{rr}b_r\\\\0\\\\\\vdots\\\\0\n\t\\end{pmatrix}~~\n\t~~~~~\t\\Leftrightarrow~~~~~~~\\begin{matrix}\n\ta_{11}^2x_1=a_{11}b_1\\\\\n\t\\vdots\\\\\n\ta_{rr}^2x_r=a_{rr}b_r\\\\\n\t0\\cdot x_i=0\\\\\n\t{(i> r)}\n\t\\end{matrix}~~\n\t~~~~~\t\\Leftarrow~~\\begin{matrix}\n\tx_1=\\frac{b_1}{a_{11}}\\\\\n\t\\vdots\\\\\n\tx_r=\\frac{b_r}{a_{rr}}\\\\\n\tx_{r+1}=0\\\\\n\t\\vdots\\\\\n\tx_n=0\n\t\\end{matrix}\\\\\n\t\\Rightarrow~~x^+=\\begin{pmatrix}\n\t\\frac{1}{a_{11}}b_1\\\\\n\t\\vdots\\\\\n\t\\frac{1}{a_{rr}}b_r\\\\\n\t0\\\\\n\t\\vdots\\\\\n\t\\end{pmatrix}\n\t0=A^+b,~~~A^+=\\begin{pmatrix}\n\t\\frac{1}{a_{11}}&~&~&~&~&0\\\\\n\t~&\\ddots&~&~&~&~\\\\\n\t~&~&\\frac{1}{a_{rr}}&~&~&~\\\\\n\t~&~&~&0&~&~\\\\\n\t~&~&~&~&\\ddots&~\\\\\n\t0&~&~&~&~&0\n\t\\end{pmatrix}\\in\\mathbb{R}^{n\\times m}\n\t\\end{align*}\n\t{\\color{cyan}\\textbf{Note:} The $x_i$ for $i>r$ can be chosen arbitrarily, but setting them to zero gives the smallest vector.}\n\t\n~\\\\\n\t\\textbf{(2) General Case}\\\\~\\\\\nBy using the SVD $A =U\\Sigma V^\\top$ we find\n$$\nA^TA=(U\\Sigma V^T)^TU\\Sigma V^T=V\\Sigma^T\\Sigma V^T,\n$$\nso that the normal equation reads as\n\\begin{align*}\n(\\ast)~~~~A^TAx=A^Tb~~&\\Leftrightarrow~~V\\Sigma^T\\Sigma (V^Tx)=V\\Sigma^T(U^Tb)\\\\\n&\\stackrel{V^T\\cdot |}{\\Leftrightarrow}~~\n\\underbrace{\\Sigma^T\\Sigma (V^Tx)=\\Sigma^T(U^Tb)}_{(\\text{normal equation for} (\\Sigma,~U^Tb) )}~~~(\\sharp)\n\\end{align*}\nConsequently, $x$ solves $(\\ast)$ if and only if $y:=V^Tx$ solves $(\\sharp)$. Since $V$ is orthogonal both solutions have the same norm, more precisely,\n$$\\|x\\|_2^2 = x^\\top x = x^\\top(V^\\top V)x = \\|Vx\\|_2^2=\\|y\\|_2^2.$$\nFor diagonal matrices we have shown that $y^+=\\Sigma^+U^Tb$ is the smallest solution of $(\\sharp)$. Thus, $x^+ := Vy^+ = \\Sigma^+U^Tb$ is the smallest solution of  $(\\ast)$, i.e., the minimum norm least squares solution.\\\\~\\\\\n\\textbf{All in all:} Since orthogonal matrices (here $U$ and $V$) are not only invertible but also isometric and the SVD $A=U\\Sigma V^\\top$ always exists, we could rely on the result for diagonal matrices (here $\\Sigma$).\n\n\n%is the minimum norm least squares solution of ``$\\Sigma \\overline{x}\\cong\\overline{b}$''. We now show that we can interchange variable transformation and computing minimum norm least squares solution, because $V$ is orthogonal.\n%\\begin{align*}\n%(*)~~\\begin{matrix}\n%\\min_{\\overline{x}\\in\\mathbb{R}^n}\\|\\overline{x}\\|_2^2\\\\\n%\\text{s.t.}~\\Sigma^T\\Sigma\\overline{x}=\\Sigma^T\\overline{b}\n%\\end{matrix}~~&=~~\n%\\begin{matrix}\n%\\min_{x\\in\\mathbb{R}^n}\\|V^Tx\\|_2^2\\\\\n%\\text{s.t.}~\\Sigma^T\\Sigma V^Tx=\\Sigma^TU^Tb\n%\\end{matrix}\\\\\n%&=~~\\begin{matrix}\n%\\min_{x\\in\\mathbb{R}^n}\\|x\\|_2^2\\\\\n%\\text{s.t.}~V\\Sigma^T\\Sigma V^Tx=V\\Sigma^TU^Tb\n%\\end{matrix}\\\\\n%&=~~\\begin{matrix}\n%\\min_{x\\in\\mathbb{R}^n}\\|x\\|_2^2\\\\\n%\\text{s.t.}~A^TAx=A^Tb\n%\\end{matrix}~~(\\sharp)\n%\\end{align*}\n%Thus: $\\overline{x}^+$ minimizes $(*)$ among all $\\{\\overline{x}=V^Tx:~x\\in\\mathbb{R}^n\\}~~\\Leftrightarrow~~x^+:=V\\overline{x}^+$ minimizes $(\\sharp)$ among all $x\\in\\mathbb{R}^n$\n%$\\Rightarrow~~x^+=V\\overline{x}^+=V\\Sigma^+U^Tb=A^+b$\n\n}\n",
    "id": ""
}