{
    "filename": "EigenvaluesProperties.tex",
    "title": "",
    "subtitle": "",
    "coding": "",
    "mathFields": "",
    "tags": "",
    "relatedExercises": "",
    "solutionLength": "",
    "task": "% !TeX spellcheck = en_US\n\\textbf{Properties of Eigenvalues}\n\n%Prove \\underline{three} (you get \\textbf{2 bonus points} for a fourth solution) of \nProve the following statements (see also the corresponding Lemma from the lecture):\n\\begin{enumerate}\n\t\\item  \\textit{The eigenvalues of the powers of a matrix:}\\\\ \n\tLet $A \\in \\F^{n\\times n} ,~\\lambda \\in \\sigma(A)$ then $\\lambda^k \\in \\sigma(A^k)~\\text{for any}~k\\in \\N$ .\n\t\\item\t\\textit{Eigenvalues of invertible Matrices:}\\\\ Let $A\\in \\mathbb{F}^{n \\times n}$ be invertible and $\\lambda \\in \\sigma(A)$, then $\\lambda \\neq 0$ and $\\frac{1}{\\lambda}$ is an eigenvalue of $A^{-1}$.\n\t\\item \\textit{Eigenvalues of a scaled matrix:}\\\\ \n\tLet $A\\in \\mathbb{F}^{n \\times n}$ and $\\lambda \\in \\sigma(A)$, then $\\alpha\\lambda \\in \\sigma(\\alpha A)$ for any $\\alpha \\in \\mathbb{F}$.\n\t\\item \\textit{Real symmetric matrices have real eigenvalues:} {\\small\\color{red}(Not obvious without using properties of complex numbers! Solution not relevant for exam and thus not discussed here.)}\\\\ $A \\in \\R^{n\\times n},~A = A^T ~~\\Rightarrow~~ \\sigma(A)\\subset\\R$.\n\t\\item \\textit{The eigenvalues of real orthogonal matrices:} {\\small\\color{red}(Not obvious without using properties of complex numbers! Solution not relevant for exam and thus not discussed here.)}\\\\\n\t$Q \\in \\Rnn$ be orthogonal, $\\lambda = a+ib \\in \\sigma(Q) ~~\\Rightarrow~~|\\lambda|:=\\sqrt{a^2+b^2}=1$\n\t\\item \\textit{The eigenvalues of an upper (or lower) triangular matrix are sitting on its diagonal:}\\\\ \n\tLet $U\\in \\mathbb{F}^{n \\times n}$ with $u_{ij} = 0$ for $i > j$. Then $\\sigma(U) = \\{u_{11}, \\ldots, u_{nn}\\}$.\n\t\\item \\textit{Similar matrices have the same eigenvalues:}\\\\ Let $A \\in \\mathbb{F}^{n \\times n}$ and $T \\in GL_n(\\mathbb{F})$, i.e., $T$ is invertible. Then $\\sigma(A) = \\sigma(T^{-1}AT)$.\n\t\\item\\textit{Eigenvalues of a shifted matrix:}\\\\ Let $A\\in \\mathbb{F}^{n \\times n}$ and $\\lambda \\in \\sigma(A)$, then $(\\lambda - \\alpha)$ is an eigenvalue of $(A - \\alpha I)$ for any $\\alpha \\in \\mathbb{F}$.\n\t\\item \\textit{Symmetric matrices have orthogonal eigenvectors:} {\\small\\color{red}(Solution not relevant for exam and thus not discussed here.)}\\\\ Let $\\lambda_1 \\neq \\lambda_2$ be two \\textit{distinct} eigenvalues of a real \\textit{symmetric} matrix $A \\in \\mathbb{R}^{n \\times n}$ (i.e., $A = A^T$), and let \n$v_1, v_2 \\in \\mathbb{R}^n$ be corresponding eigenvectors. Proof that $v_1$ and $v_2$ are orthogonal, i.e., $v_1^\\top v_2 = 0$.\n\\end{enumerate}\n",
    "solution": "{\n\t\\color{solution}\n\\begin{enumerate}\n\t\\item  \\textbf{\\textit{The eigenvalues of the powers of a matrix:}}\\\\\n\tFor an eigenpair $(\\lambda,v)$ of $A$ we find\n \\begin{align*}\n A^kv = A^{k-1}Av =  A^{k-1}\\lambda v.\n \\end{align*}\n Iterating $k$ times gives the desired result.\n %\n %\n\t\\item\t\\textbf{\\textit{Eigenvalues of invertible Matrices:}}\\\\ \n\t Let $A\\in GL_n(\\mathbb{F})$ and $\\lambda\\in\\sigma(A)$ with eigenvector $v\\neq 0$.\\\\\n\tFirst, we show that $\\lambda\\neq 0$ holds. For this purpose let us assume $\\lambda = 0$. Then\n\t$Av=\\lambda v=0$ implies $v=A^{-1}\\cdot 0=0$, which contradicts that $v$ is an eigenvector (and therefore nonzero).\\\\\n\tSecond, we proof $\\frac{1}{\\lambda}\\in\\sigma(A^{-1})$. We find\n\t\\begin{align*}Av=\\lambda v~~\\stackrel{\\textcolor{blue}{A\\in GL_n(\\mathbb{F})}}{\\Leftrightarrow}~~ v = \\lambda A^{-1}v~~\\stackrel{\\textcolor{blue}{\\lambda\\neq 0}}{\\Leftrightarrow}~~\\frac{1}{\\lambda}v=A^{-1}v~~\\Leftrightarrow~~\\frac{1}{\\lambda}\\in\\sigma(A^{-1})~~\\textcolor{blue}{(\\text{with the same eigenvector}~v).}\n\t\\end{align*}\n%\n%\n%\n\t\\item \\textbf{\\textit{Eigenvalues of a scaled matrix:}}\\\\\n\tLet $\\alpha \\in \\F$, then for an eigenpair $(\\lambda,v)$ of $A$ we find\n\t\\begin{align*}\n\tAv = \\lambda v & \\Leftrightarrow (\\alpha A) v = (\\alpha \\lambda)  v\n\t\\end{align*} \n\timplying that $(\\alpha \\lambda, v)$ is an eigenpair of $\\alpha A$. \\\\ \n\n\n\t\\item \\textbf{\\textit{Real symmetric matrices have real eigenvalues:}}\\\\ \n\tWe first collect some observations:\n\t\\begin{itemize}\n\t\t\\item In general, for $x,y\\in \\F^n$ and a matrix $A \\in \\Fnn$ we easily find by the definition of the matrix product\n\t\t\\begin{align*}\n\t\tx^\\top A y = \\sum_{i,j}a_{ij}x_i y_j .\n\t\t\\end{align*}\n\t\tIf the matrix is symmetric, i.e., $a_{ij}= a_{ji}$, we further find\n\t\t\\begin{equation} \\label{eq:symEnergy}\n\t     \tx^\\top A y = \\sum_{i}a_{ii}x_iy_i  +  \\sum_{i \\neq j}a_{ij} (x_i y_j  + x_jy_i).\n\t\t\\end{equation}\n\t\t\\item For $z=x+iy \\in \\C$ let us define $\\overline{z} := x-iy$ (the so-called \\textit{complex conjugate} of $z$). Then we easily find\n\t\t\\begin{itemize}\n\t\t\t\\item[i)] $z \\overline{z} = a^2 + b^2 \\in \\R$ (real number!),\n\t\t\t\\item[ii)] for $w = c +id$ we find $\\overline{z}w + z \\overline{w} = 2(ac+bd) \\in \\R$ (real number!) and also $\\overline{z\\cdot w} = \\overline{z}\\cdot\\overline{w}$.\n\t\t\\end{itemize}\n\t\\end{itemize}\n\tNow to the task: Let $(\\lambda, v)$ be an eigenpair of $A=A^\\top \\in \\Rnn$, i.e., \n\t$$Av = \\lambda v. $$\n\tMultiplying $\\overline{v} := (\\overline{v}_1,\\ldots, \\overline{v}_n)$ from the left yields\n\t\t$$\\overline{v}^\\top Av = \\lambda \\overline{v}^\\top v. $$\n\tNow, if we can show that $\\overline{v}^\\top Av$ and $\\overline{v}^\\top v$ are real, then we know that $\\lambda$ is real. First, we observe that \n\t$$\\overline{v}^\\top v = \\sum_{i=1}^n  \\overline{v}_i v_i,$$\n\twhich is real, since all summands $ \\overline{v}_i v_i$ are real by i) above. Secondly, since $A$ is \\textbf{symmetric} we can apply \\eqref{eq:symEnergy} to obtain\n\t\\begin{align*}\n\t \\overline{v}^\\top Av = \\sum_{i} a_{ii}\\overline{v}_i v_i +  \\sum_{i \\neq j}a_{ij} (\\overline{v}_i v_j  + \\overline{v}_jv_i),\n\t\\end{align*}\n\twhich is real, since all summands are real by i) and ii) above and the assumption that the matrix only has \\textbf{real} coefficients.\n~\\\\~\\\\\n\t\\textit{Remarks:} \\begin{itemize}\n\t\t\\item \\textbf{We cannot relax symmetry assumption:} Matrices with just real coefficients can have complex eigenvalues. Take for example the (orthogonal) matrix\n\t$$\n\tA = \\begin{pmatrix}\n\t\t0&-1\\\\\n\t\t1&0\n\t\\end{pmatrix}\n\t$$\n\tfor which $\\det(A-\\lambda I) = \\lambda^2 +1$, so that $\\sigma(A)=\\{i,-i\\}$  with eigenvectors $\\lbrace \\begin{pmatrix}\n\t1\\\\ -i\n\t\\end{pmatrix}, \\begin{pmatrix}\n\t1\\\\ i\n\t\\end{pmatrix}\\rbrace$. However, the additional property of symmetry is a sufficient condition for a real matrix $A$ to have solely real eigenvalues! \n\t\\item \\textbf{We cannot relax assumption of real coefficients:} A symmetric matrix with complex coefficients can have complex eigenvalues. Consider for example \n\t\t$$\n\tA = \\begin{pmatrix}\n\t0&i\\\\\n\ti&0\n\t\\end{pmatrix},\n\t$$\n\tfor which $\\det(A-\\lambda I) = \\lambda^2 +1$, so that $\\sigma(A)=\\{i,-i\\}$.\n\t\\item \\textbf{The general result for complex matrices:} Like \\textit{symmetry} for real matrices we introduce for complex matrices: A matrix $A \\in \\C^{n\\times n}$ is called \\textit{Hermitian} or \\textit{self-adjoint} if $A^\\top = \\overline{A}$. With other words, Hermitian matrices are invariant under transposition \\textit{and} (additionally) conjugation. With the same proof as above one can show that such matrices also have real eigenvalues. For example, consider the Hermitian matrix\n\t\t\t$$\n\tA = \\begin{pmatrix}\n\t0&-i\\\\\n\ti&0\n\t\\end{pmatrix},\n\t$$\n    for which $\\det(A-\\lambda I) = \\lambda^2 -1$, so that $\\sigma(A)=\\{1,-1\\}$.\n\t\\end{itemize}\n\t%\n\t%\n\t\\item \\textbf{\\textit{The eigenvalues of real orthogonal matrices:}}\\\\\n%\t{\\small\\color{red}(Note that we have introduced the notion of \\textit{orthogonality} only for real matrices and vectors!)}\\\\\n\tWe know $Q^\\top Q = I$.  Now let $(\\lambda, v)$ be an eigenpair of $Q$, i.e., $Qv = \\lambda v$. Using the notation from the previous subtask, i.e., letting $\\overline{v}  = (\\overline{v}_1,\\ldots, \\overline{v}_n)$ denote the complex conjugate of $v$, we find on the one hand that\n\t$$(Q \\overline{v})^\\top (Qv) = \\overline{v}^\\top Q^\\top Q v = \\overline{v}^\\top v  \\in \\R.$$\n \tOn the other hand, since $Q$ is assumed to be real we find\n\t\t$$(Q \\overline{v})^\\top (Qv)=(\\overline{Q v})^\\top (Qv) =(\\overline{ \\lambda v})^\\top ( \\lambda v)  =\\overline{ \\lambda} \\lambda  (\\overline{v}^\\top v ) = |\\lambda |^2 (\\overline{v}^\\top v )\\in \\R.$$\n    Thus combining these two equations gives\n    $$|\\lambda | = 1. $$\n%    \\textit{Remark:} Due to the general result $\\|Qv\\|_2 = \\|v\\|_2$ derived above, orthogonal matrices are \\textit{isometric}, i.e., they do not change the Euclidean norm (=length) of a vector.\n    %\n    %\n    %\n\t\\item \\textbf{\\textit{The eigenvalues of an upper (or lower) triangular matrix are sitting on its diagonal:}}\\\\ \n\t\t\\underline{Recall:} \\\\\n1) The determinant of an (upper) triangular matrix is given by the product of its diagonal entries\n$$\n\\text{det}(U)=u_{11}\\cdot u_{22}\\cdot\\ldots\\cdot u_{nn}.\n$$\n2) The eigenvalues $\\lambda$ of a matrix $A\\in\\mathbb{R}^{n\\times n}$ are the roots of the characteristic polynomial \n$$\n\\text{det}(A-\\lambda I) = 0.\n$$\nSince $U-\\lambda I = \\begin{pmatrix}u_{11}-\\lambda& &*\\\\ &\\ddots& \\\\0& &u_{nn}-\\lambda\\end{pmatrix}$ is also upper triangular we find\n$$\n\\text{det}(U-\\lambda I)=(u_{11}-\\lambda)\\cdot(u_{22}-\\lambda)\\cdot\\ldots\\cdot(u_{nn}-\\lambda)\\stackrel{!}{=}0\\Leftrightarrow \\lambda\\in\\{u_{11},\\dots,u_{nn}\\}.\n$$\nAnalogue proof for lower triangular matrices.\n%\n%\n\t\\item \\textbf{\\textit{Similar matrices have the same eigenvalues:}}\\\\ \n\t\tLet $(\\lambda,v)$ be an eigenpair of $A$, then since $T$ is invertible we find\n$$\n\\underbrace{Av=\\lambda v}_{\\textcolor{blue}{\\text{by definition of} (\\lambda,v)}}\\stackrel{\\textcolor{blue}{T^{-1}\\cdot|}}{\\Leftrightarrow} \\underbrace{T^{-1}Av}_{\\textcolor{blue}{=T^{-1}AIv}} = T^{-1}(\\lambda v) \\stackrel{\\textcolor{blue}{I=TT^{-1}}}{\\Leftrightarrow} T^{-1}AT(T^{-1}v) = \\lambda(T^{-1}v).\n$$\n\tThus, $(\\lambda, T^{-1}v)$ is an eigenpair for the matrix $T^{-1}AT$ (note that $T^{-1}v\\neq 0$, since $v\\neq 0$).\\\\~\\\\\n\t\\textit{Remark:} We call two matrices $A$ and $B$ \\textit{similar} if there exists an invertible matrix $T$ such that $B = T^{-1}AT$.\n%\n%\n%\n\t\\item\\textbf{\\textit{Eigenvalues of a shifted matrix:}}\\\\ \n\t\tLet $\\alpha\\in\\mathbb{F}$ and $(\\lambda,v)$ be an eigenpair of $A$. Then $((A-\\alpha I)-(\\lambda -\\alpha)I)v=(A-\\lambda I)v\\stackrel{\\textcolor{blue}{\\lambda\\in\\sigma(A)}}{=}0$\\\\\n\t Thus ${(\\lambda-\\alpha)~\\text{is an eigenvalue of}~(A-\\alpha I)~\\text{with the same eigenvector}~v}$.\n\t\t%\n\t\t%\n\t\\item \\textbf{\\textit{Symmetric matrices have orthogonal eigenvectors:}}\\\\ \n\t\\textit{Remark upfront:} Since we are considering a real symmetric matrix, we know that the eigenvalues are real. However, one may still find complex eigenvectors. For example consider the identity matrix $I$ with spectrum $\\sigma(I) = \\{1\\}$. Then obviously any nonzero vector $v$ (complex or not) is an eigenvector to the eigenvalue $1$. We now show that real eigenvalues enable us to choose real eigenvectors (as implicitly assumed in the task). To clarify this, let $(\\lambda,v)$ be an eigenpair, where $\\lambda \\in \\R$ but $v$ may potentially have complex coefficients. Let us split $v$ according to the real and imaginary parts of its coefficient, more precisely\n\t $$v = \\begin{pmatrix}\n\t v_1\\\\\\vdots\\\\v_n\n\t \\end{pmatrix}\n\t =  \\begin{pmatrix}\n\t x_1 + i y_1\\\\\\vdots\\\\ x_n + i y_n\n\t \\end{pmatrix} = \\begin{pmatrix}\n\t x_1 \\\\\\vdots\\\\ x_n\n\t \\end{pmatrix}+ i \\begin{pmatrix}\n\t y_1\\\\\\vdots\\\\  y_n\n\t \\end{pmatrix} =: x + i y.$$\n\t Then we find \n\t $$Av = \\lambda v \\Leftrightarrow  A(x+iy) = \\lambda (x+iy) \\Leftrightarrow Ax + i Ay = \\lambda x + i \\lambda y .$$ Note here that $\\lambda$ is real and thus we have the splitting into real $\\lambda x $ and imaginary part $\\lambda y$. By comparing real parts in the last equation we obtain $$Ax = \\lambda x, $$ so that the real part of $v$ is also an eigenvector to the eigenvalue $\\lambda$.\n\t~\\\\\n\t\n\t\\textit{Now to the task:} Let $A$ be symmetric, i.e., $A=A^T$ and let $(\\lambda_1,v_1)$ and $(\\lambda_2,v_2)$ be two (real) eigenpairs of $A$ with $\\lambda_1\\neq\\lambda_2$, then\n\t$$\n\t\\textcolor{red}{v_1^T}\\underbrace{\\textcolor{red}{Av_2}}_{\\textcolor{blue}{=\\lambda_2v_2}} = \\lambda_2v_1^Tv_2,\n\t$$ \\text{and also}\n\t$$ \\textcolor{red}{v_1^T}\\underbrace{\\textcolor{red}{A}}_{\\textcolor{blue}{=A^T}}\\textcolor{red}{v_2} = \\underbrace{v_1^TA^Tv_2}_{\\textcolor{blue}{=(v_1^TA^Tv_2)^T}} = v_2^T\\underbrace{Av_1}_{\\textcolor{blue}{=\\lambda_1v_1}} = \\lambda_1v_2^Tv_1 = \\lambda_1v_1^Tv_2. \n\t$$\n\tNow we subtract these terms and find \n\t$$\n\t0=\\textcolor{red}{v_1^TAv_2-v_1^TAv_2} = \\lambda_2v_1^Tv_2 - \\lambda_1v_1^Tv_2 = \\underbrace{\\lambda_2-\\lambda_1}_{\\textcolor{blue}{\\neq 0,\\  \\text{since}\\ \\lambda_2\\neq\\lambda_1}}v_1^Tv_2\n\t$$\n\t$$\n\t\\stackrel{\\textcolor{blue}{\\lambda_1\\neq\\lambda_2}}{\\Rightarrow} v_1^Tv_2 = 0.\n\t$$\n\\end{enumerate}\n\n\n}",
    "id": ""
}