{
    "filename": "info-PageRank.tex",
    "title": "",
    "subtitle": "",
    "coding": "",
    "mathFields": "",
    "tags": "Linear Algebra, Eigenvalues",
    "relatedExercises": "",
    "solutionLength": "",
    "task": "{\\color{navy}\n\\textbf{An Application of Eigenvectors: The \\textit{PageRank}}\n\n\t\\textbf{Aim:} To rank results of a web search engine (such as Google) according to the \\textit{``importance''} of the web pages.\\\\\n\t\n\t\\textbf{1998:} For this purpose, Larry Page and Sergei Brin developed the PageRank algorithm as the basis of the \n\t\\raisebox{-0.25\\baselineskip}{\\includegraphics[width=1.2cm]{\\PathToMedia/7_Google.png}} empire.\\\\ \n\t\n\t\\textbf{Assumption:} \\textit{``important''} means more links from other (important) web pages.\\\\\n\n\n{\\bf Idea:} \nLet us think of the web as a directed graph, i.e., web pages are nodes and links from one page to another, i.e, from one node to another, are modeled as directed edges. For example a web structure consisting of 11 web pages could look as follows: \n\\begin{center}\n\t\\begin{minipage}[c]{0.5\\textwidth}\n\t\t%\tPotential Web Structure:\n\t\t\\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.9cm,\tsemithick]\n\t\t\\tikzstyle{every state}=[fill=red,draw=none,text=white]\t\n\t\t\\node[state,fill=blue] (A)                   {$1$};\n\t\t\\node[state,scale=2.2] (B)    [right of=A]               {$2$};\n\t\t\\node[state,fill=yellow,scale=1.8] (C)    [right of=B]    {\\color{black}$3$};\n\t\t\\node[state,fill=green] (D)    [below of=A]               {\\color{black}$4$};\n\t\t\\node[state,fill=brown] (E)    [below left of =C]               {$5$};\n\t\t\\node[state,fill=green] (F)    [below of=C]               {\\color{black}$6$};\n\t\t\\node[state,fill=purple,scale=0.7] (G)    [below of=D]               {$7$};\n\t\t\\node[state,fill=purple,scale=0.7] (H)    [right of=G]               {$8$};\n\t\t\\node[state,fill=purple,scale=0.7] (I)    [right of=H]               {$9$};\n\t\t\\node[state,fill=purple,scale=0.7] (J)    [right of=I]               {$10$};\n\t\t\\node[state,fill=purple,scale=0.7] (K)    [right of=J]               {$11$};\t\n\t\t\\path (B) edge [bend left=15pt]  (C)\n\t\t(C) edge [bend left=15pt]  (B);\n\t\t\\path (D) edge   (A)\n\t\t(D) edge   (B);\n\t\t\\path (E) edge   (D)\n\t\t(E) edge (B);\n\t\t\\path (F) edge [bend left=20pt]  (E)\n\t\t(E) edge [bend left]  (F)\n\t\t(E) edge  (B);\n\t\t\\path (G) edge   (B)\n\t\t(G) edge [bend left=5pt] (E);\n\t\t\\path (H) edge   (B)\n\t\t(H) edge (E);\n\t\t\\path (I) edge   (B)\n\t\t(I) edge [bend right=5pt](E);\n\t\t\\path (J) edge [bend right=5pt](E);\n\t\t\\path (K) edge [bend left=2pt](E);\t\n\t\t\\end{tikzpicture}\n\t\t\n\t\\end{minipage}\n\\end{center}\n\nWe now randomly place a random surfer according to the initial probability distribution $x^0 = (x^0_1, \\ldots, x_n^0)$ on this graph. Here, $n$ (in the example above $n=11$) denotes the number of web pages and $x^0_i$ denotes the probability that the random surfer \\textit{starts} at web page $i$. Further let $e := (1,\\ldots,1)^T$, then the fact that $x^0$ is a probability distribution (i.e., probabilities sum up to $1$) translates into $e^Tx^0 =x^0_1+\\ldots+x_n^0 =1$. Now we make the assumption that the random surfer moves...\n\\begin{itemize}\n\t\\item[(1)] ...with probability $\\alpha \\in (0,1)$ according to the link structure (with equal preferences to outgoing links)\n\t\\item[(2)] ...with probability $(1-\\alpha)$ he can teleport to a random page (with equal probability) to prevent stranding in deadlocks\n\t\\item[$\\rightarrow$] Pages, where the random surfer is more likely to appear in the long run based on the web's structure are considered more important. \n\\end{itemize}\n\nThese two movements can be described by multiplying the current probability distribution with the two following matrices:\n$$P_1={ \n\t\\left(  \\begin{tabular}{cccccccccccc}\n\t~ & 1    & 2   & 3    & 4    & 5    & 6 & 7     & 8   & 9  & 10   & 11    \\\\\n\t1 & 1    &~     &  ~   & 1/2  &    ~ & ~ & ~     &  ~  &  ~   & ~ &    ~  \\\\\n\t2 & ~    & ~    &  1   & 1/2  & 1/3  &   & 1/2   & 1/2 & 1/2 &   &    \\\\\n\t3 & ~    & 1    &  ~   &      &      &   &       &     &     &   &    \\\\\n\t4 & ~    &  ~   & ~    &      &  1/3 &   &       &     &     &   &  \\\\  \n\t5 & ~    &  ~   & ~    &      &      & 1 & 1/2   & 1/2 & 1/2 & 1 & 1 \\\\  \n\t6 & ~    &  ~   &  ~   &      & 1/3  &   &       &     &     &   &  \\\\  \n\t7 & ~    & ~    &  ~   &      &      &   &       &     &     &   &  \\\\  \n\t8 &~     & ~    &  ~   &      &      &   &       &     &     &   &  \\\\  \n\t9 & ~    &  ~   &  ~   &      &      &   &       &     &     &   &  \\\\  \n\t10 & ~    &  ~   & ~    &      &      &   &       &     &     &   &  \\\\  \n\t11 & ~    &  ~   & ~    &      &      &   &       &     &     &   &     \n\t\\end{tabular} \\right)\n},\n~~~~P_2 := \\frac{1}{n}ee^T= \\left(\\frac{1}{n}\\right)_{ij}$$\nMore precisely,\n\\begin{itemize}\n\t\\item[(1)] \\textbf{Link structure:} $P_1$ is the probability matrix (column stochastic) defined by\\\\\n\t$P_1^{ij}:=$  Probability that random surfer moves from page $j$ to page $i$ defined by the link structure\n\t\\item[(2)] \\textbf{Jumps:}  $P_2$ is the probability matrix (column stochastic) defined by\\\\\n\t$P_2^{ij} := \\frac{1}{n}$= Probability that random surfer jumps from page $j$ to page $i$\n\\end{itemize}\nThe movement of the random surfer is then completely defined by the probability matrix \n$$P = \\alpha P_1 + (1-\\alpha)P_2 .$$\nThis matrix is also known as the \\textit{\\textbf{Google Matrix}}. For the next time instances we therefore obtain\n\\begin{align*}\nx^1 &=  \\alpha P_1 x^0 + (1-\\alpha) P_2 x^0 = Px^0  \\\\\nx^2 &=  \\alpha P_1 x^1 + (1-\\alpha) P_2 x^1 = Px^1 \\\\[0.1cm]\nx^{k+1} &=  \\alpha P_1 x^k + (1-\\alpha) P_2 x^k = Px^k = P^{k+1}x^0 \\\\[0.1cm]\nx^* &=  \\lim_{k\\to \\infty} x^k =: \\textbf{\\textit{PageRank}} \n\\end{align*} \n\\textbf{Observations:}\n\\begin{itemize}\n\t\\item One can easily show that $P_1$, $P_2$ and thus $P$ are column stochastic (i.e., $e^TP = e^T$)\n\t\\item Consequently, since $x^0$ is a probability distribution (i.e., $e^Tx^0 = 1$), also $e^Tx^k = 1$ for all $k$ and $e^Tx^*=1$\n\\end{itemize}\n\n\\textbf{Question:} \\\\\nDoes this sequence $\\{x^k\\}_{k \\in \\N}$ of vectors converge (to a steady state)? More precisely, is there a $x^* = \\lim_{k\\to \\infty} x^k= \\lim_{k\\to \\infty} P^kx^0$, so that\n\\begin{equation} \\label{eq:PageRank_eigprob}\nPx^* = 1 x^* .\n\\end{equation}\n\\begin{itemize}\n\t\\item[$\\rightarrow$] With other words, is there an \\textit{\\textbf{eigenvector}} $x^*$ to the \\textit{\\textbf{eigenvalue}} $1$ of the matrix $P$?\n\t\\item[$\\rightarrow$]\\textit{\\textbf{Eigenvalue algorithms}} are developed to solve such problems. One of them is the \\textit{\\textbf{Power iteration}}, which, applied to the eigenvalue problem above, produces precisely the sequence\n\t$$x^k = P^k x^0.$$\n\\end{itemize}\nIntuitively, in the limit most of the ``mass'' would be located at web pages that have many incoming links and would therefore be ranked as being more important. In fact, the $i$-th component of $x^*$ is called the \\textit{PageRank} of the web page $i$.\\\\~\\\\\nRemark: \\textit{Perron Theorem}\\\\\nA positive damping factor $\\alpha>0$ is also technically necessary as it assures that the matrix $P$ has only strictly positive coefficients. The Perron Theorem then sates that its largest eigenvalue is strictly larger than all other eigenvalues (in magnitude). Thus the convergence of the Power method is guaranteed. Since the matrix is column stochastic one can further show that the largest eigenvalue is $1$.\n}",
    "solution": "",
    "id": ""
}