{
    "filename": "Questions_all.tex",
    "title": "",
    "subtitle": "",
    "coding": "",
    "mathFields": "",
    "tags": "",
    "relatedExercises": "",
    "solutionLength": "",
    "task": "\nAnswer the following questions.\n\\begin{enumerate}\n\t\\item How is a \\textit{norm} $L: \\mathbb{F}^n \\to [0, \\infty)$ defined?\n\t%\n\t\\item Assume you are given the singular value decomposition \n\t$U \\Sigma V^\\top = A$ of some matrix $A \\in \\mathbb{R}^{m \\times n}$, where the \n\tentries on the diagonal of $\\Sigma$ are given in a descending order.\n\tDenote the best rank-$k$ approximation of $A$ for $k \\leq \\min\\{m,n\\}$ and denote the criterion with respect to which this is the best approximation.\n\t%\n\t\\item Give an example for a matrix $A \\in \\mathbb{R}^{2 \\times 2}$ where the LU-decomposition algorithm\n\tnecessarily needs a permutation step.\n\t%\n\t\\item Let $\\lambda_1 \\neq \\lambda_2$ be two eigenvalues of a \\textit{symmetric} matrix $A \\in \\mathbb{R}^{n \\times n}$, and let \n\t$v_1, v_2 \\in \\mathbb{R}^n$ be corresponding eigenvectors. Proof that $v_1^\\top v_2 = 0$.\n\t%\n\t\\item How is positive definiteness of a matrix $A\\in \\mathbb{R}^{n \\times n}$ defined? What does this mean for the angle\n\tbetween a vector $x \\in \\mathbb{R}^{n}$ and the vector $z := Ax$?\n\t%\n\t\\item How is \\textit{injectivity} of a function $f\\colon X \\to Y$ defined?\n\t%\n\t\\item How is a \\textit{scalar product} $P\\colon \\mathbb{R}^n \\times \\mathbb{R}^n \\mapsto \\mathbb{R}$ defined?\n\t%\n\t%\n\t\\item Assume you are given the singular value decomposition (SVD) $U \\Sigma V^\\top = A$ of some matrix $A \\in \\mathbb{R}^{m \\times n}$. What is the singular value decomposition of $A^\\top$? Give a basis for $\\Im(A)$ and $\\Im(A^\\top)$.\n\t%\n\t\\item Assume you are given the singular value decomposition (SVD)\n\t$U \\Sigma V^\\top = A$ of some matrix $A \\in \\mathbb{R}^{m \\times n}$. Determine the pseudoinverse $A^+$ of $A$ with the help of the SVD. Explain why $A^+ = A^{-1}$, if $A$ is invertible (\\textit{hint:} note that $m=n$ in this case).\n\t%\n\t\\item Let $A \\in \\mathbb{R}^{n \\times n}$ be a matrix. Are the notions of\n\tinjectivity and surjectivity of $A$ equivalent? Give a short justification.\n\t%\n\t\\item What is the normal equation? Where is it applied?\n\t%\n\t\\item Give an example of a vector space other than $\\mathbb{R}^n$? \n\t%\n\t\\item Let $V$ be a vector space over the field $\\mathbb{F}$. Give the definition of a basis.\n\t%\n\t\\item Draw the sets $\\{x \\in \\mathbb{R}^{2} \\colon \\|x\\|_p = 1   \\}$ for $p=1,2,\\infty$.\n\t%\n\t\\item What is the purpose of the power method? Write down its iteration instruction.\n\t%\n\t\\item What is the purpose of the Richardson iteration? Write down its iteration instruction. When does it converge?\n\t%\n\t\\item Let $R=(r_{ij})_{ij} \\in \\mathbb{R}^{n \\times n}$ be a (lower or upper) triangular matrix with $r_{nn} = 0$. Is $R$ invertible? Explain your answer.\n\t%\n\t\\item Let $A \\in \\mathbb{R}^{n \\times n}$ be a symmetric matrix. Are its singular values equal to its eigenvalues?\n\t%\n\t\\item Let  $A \\in \\mathbb{R}^{m \\times n}$ have independent columns. How can we use the $QR$-decomposition of $A$ to solve the least squares problem $\n\t\\min\\limits_{x \\in \\mathbb{R}^n } \\| Ax - b \\|_2^2.\n\t$, where $b \\in \\mathbb{R}^{m}$.\n\t%\n\t\\item How is the rank of a real matrix $A \\in \\mathbb{R}^{m \\times n}$ defined?\n%\n\t\\item What does the dimension formula say?\n%\n\t\\item Denote the optimization problem which is related to the principal component analysis.\n\t%\n\t\\item When is a diagonal matrix invertible? Write down the inverse in this case.\n\t%\n\t\\item What is the purpose of the $QR$ Algorithm? Write down its iteration instruction.\n\t%\n\t\\item Consider the iteration $x_{k+1} = Mx_k + b$ for some matrix $M \\in \\mathbb{R}^{n \\times n}$ and vector $b \\in \\mathbb{R}^n$. Name a sufficient condition for the convergence of this sequence. What is the limit in this case?\n\t%\n\t\\item What is the definition of an orthogonal matrix? What does it mean for the columns of the matrix?\n\t%\n\\end{enumerate}\n",
    "solution": "{\\color{solution}\n\\begin{enumerate}\n\t\\item $~~\\textcolor{exampoints}{(2P)}~~L:\\mathbb{F}^n\\rightarrow[0,+\\infty)$ norm: $\\Leftrightarrow$\n\t\\begin{align*}\n\t&\\text{i)}~L(x)=0~~\\Rightarrow~~x=0\\\\\n\t&\\text{ii)}~L(\\lambda x)=|\\lambda|L(x)\\\\\n\t&\\text{iii)}~L(x+y)\\leq L(x)+L(y)\n\t\\end{align*}\n\t%\n\t\\item \n\t$A=U\\Sigma V^T=\\sum_{j=1}^{\\text{min}(n,m)}\\sigma_ju_jv_j^T$\\\\\n\tbest rank-k approximation is given by truncated SVD\n\t\\begin{align*}\n\t&\\textcolor{exampoints}{(1P)}~~A_k:=\\sum_{j=1}^{k}\\sigma_ju_jv_j^T~\\text{for which}\\\\\n\t&\\textcolor{exampoints}{(1P)}~~A_k:=\\min_{B,\\text{rank}(B)=k}\\|B-A\\|_F^2.\n\t\\end{align*}\n\t%\n\t\\item $~~\\textcolor{exampoints}{(2P)}~~A=\\begin{pmatrix}0&1\\\\1&1\\end{pmatrix}$\n\t%\n\t\\item $~~\\textcolor{exampoints}{(2P)}~~A\\in\\mathbb{R}^{n\\times n}~\\text{symmetric}~, \\lambda_1\\neq\\lambda_2\\in\\sigma(A)~\\text{with}~v_1, v_2$\n\t\\begin{align*}\n\t\\Rightarrow~~&v_1^TAv_2=\\lambda_2v_1^Tv_2\\\\\n\t\\text{and}~~&v_1^TAv_2=v_2^TA^Tv_1=\\lambda_1v_1^Tv_2\\\\\n\t\\Rightarrow~~&(\\lambda_1-\\lambda_2)v_1^Tv_2=0\\\\\n\t\\stackrel{\\textcolor{blue}{\\lambda_1\\neq\\lambda_2}}{\\Rightarrow}~~&v_1^Tv_2=0\n\t\\end{align*}\n\t%\n\t\\item \n\t$A\\in\\mathbb{R}^{n\\times n}$ positive definite $:\\Leftrightarrow~~\\forall x\\in\\mathbb{R}^n\\setminus\\{0\\}:~x^TAx>0$\n\t\\begin{align*}\n\t&\\Rightarrow~~0<x^T\\underbrace{(Ax)}_{\\textcolor{blue}{:=z}}=\\text{cos}(\\alpha)\\underbrace{\\|x\\|\\|Ax\\|}_{\\textcolor{blue}{\\geq 0}}~~\\textcolor{exampoints}{(1P)}\\\\\n\t&\\Rightarrow~~\\text{cos}(\\alpha)>0~~\\Rightarrow~~\\alpha\\in\\left(-\\frac{\\pi}{2},\\frac{\\pi}{2}\\right),~|\\alpha|<90^{\\text{o}}~~\\textcolor{exampoints}{(1P)}\n\t\\end{align*}\n\t%\n\t\\item $~~\\textcolor{exampoints}{(1P)}~~f:X\\rightarrow Y~\\text{injective}~~:\\Leftrightarrow~~f(x)=f(y)~~\\Rightarrow~~x=y~~\\forall x,y\\in X$\n\t%\n\t\\item $P:\\mathbb{R}^n\\times\\mathbb{R}^n\\rightarrow \\mathbb{R}$ scalar product, if\n\t\\begin{align*}\n\t&\\textcolor{exampoints}{(1P)}~~\\text{i)}~~\\forall x,y\\in\\mathbb{R}^n:~P(x,y)=P(y,x)\\\\\n\t&\\textcolor{exampoints}{(1P)}~~\\text{ii)}~~\\forall x\\in\\mathbb{R}^n\\setminus\\{0\\}:~P(x,x)>0\\\\\n\t&\\textcolor{exampoints}{(1P)}~~\\text{iii)}~~\\forall x,y,z\\in\\mathbb{R}^n:~P(x,y+z)=P(x,y)+P(x,z)\\\\\n\t&\\textcolor{exampoints}{(1P)}~~\\text{iv)}~~\\forall x,y\\in\\mathbb{R}^n,\\lambda\\in\\mathbb{R}:~P(x,\\lambda y)=\\lambda P(x,y)\n\t\\end{align*}\n\t%\n\t\\item \n\t\\begin{itemize}\n\t\t\\item \n\t\t$\\textcolor{exampoints}{(1P)}$ pseudoinverse: $A^+=V\\Sigma^+U^T$, where $\\Sigma^+=\\text{diag}(\\frac{1}{\\sigma_i}:~\\sigma_i\\neq 0)$\n\t\t\\item \n\t\tLet $A\\in GL_n(\\mathbb{R})$, then $\\textcolor{exampoints}{(1P)}~\\sigma_{ii}\\neq 0~\\forall i$ and $A^{-1}=(U\\Sigma V^T)^{-1}=V\\Sigma^{-1}U^T~\\textcolor{exampoints}{(1P)}~\\\\\n\t\t(V,U~\\text{orthogonal})$. Since $\\Sigma^{-1}=~\\text{diag}(\\frac{1}{\\sigma_i})=\\Sigma^+$ we find $A^{-1}=A^+~~\\textcolor{exampoints}{(1P)}$\n\t\\end{itemize}\n\t%\n\t\\item \n\t%\n\t\\item \n\t%\n\t\\item $~~\\textcolor{exampoints}{(1P)}~~\\mathbb{R}^{m\\times n},~~P_n(\\mathbb{R}):=\\{x\\mapsto\\sum_{i=0}^{n}\\alpha_ix^i:~(\\alpha_0,\\dots,\\alpha_n)\\in\\mathbb{R}^{n+1}\\}$\n\t%\n\t\\item \n\t$\\{v_1,\\dots,v_n\\}\\subset V$ basis :$\\Leftrightarrow$\n\t\\begin{align*}\n\t&\\textcolor{exampoints}{(1P)}~~\\text{i)}~~\\{v_1,\\dots,v_n\\}~\\text{linearly independent}~~(\\Leftrightarrow~\\sum \\lambda_jv_j=0~\\Rightarrow~\\lambda_j=0~\\forall~j\\\\\n\t&\\textcolor{exampoints}{(1P)}~~\\text{ii)}~~\\text{span}(v_1,\\dots,v_n)=V~~\\textcolor{blue}{(\\Leftrightarrow~\\{\\sum \\lambda_jv_j:~\\lambda_j\\in\\mathbb{R}\\}=V)}\n\t\\end{align*}\n\t%\n\t\\item \n\t\\includegraphics[width=0.25\\textwidth]{norms.pdf}\t\n\t\\item \n\t$\\textcolor{exampoints}{(2P)}~$ The power iteration is an algorithm to find an eigenvector of a matrix\\\\ $A\\in\\mathbb{R}^{n\\times n}$ corresponding to its largest eigenvalue. \n\tThe drawback of this algorithm is the previous mentioned fact, that the resulting eigenvector is corresponded to the largest eigenvalue.\\\\\n\tThe iterations are computed by $x^{k+1}:=\\frac{Ax^k}{\\|Ax^k\\|}$.\\\\\n\tAs an alternative you can use the Inverse Power iteration.\n\t%\n\t\\item \n\t$\\textcolor{exampoints}{(2P)}~$ $R$ is not invertible, because triangular matrices are invertible if and only if all diagonal entries are nonzero (see backward/forward substitution)\n\t%\n\t\\item \n\t$\\textcolor{exampoints}{(2P)}~$ No!\n\t\\begin{align*}\n\t&\\lambda_i\\in\\sigma(A^TA),~\\sigma_i:=\\sqrt{\\lambda}=|\\tilde{\\lambda}_i|\\\\\n\t&\\tilde{\\lambda}_i\\in\\sigma(A)~~\\Rightarrow~~\\tilde{\\lambda}_i=\\lambda_i\\in\\sigma(A^TA)=\\sigma(A^2)\n\t\\end{align*}\n\t\\underline{Thus:} They are only equal up to the sign.\n\t%\n\t\\item \n\t$\\textcolor{exampoints}{(2P)}~$ Insert $A=QR$ into the normal equation:\n\t\\begin{align*}\n\tA^TAx=A^Tb~~&\\Leftrightarrow~~(QR)^TQRx=(QR)^Tb~~\\Leftrightarrow~~R^TRx=R^TQ^Tb\\\\\n\t&\\stackrel{\\textcolor{blue}{[R^T~\\text{invertible, since A has independent columns}]}}{\\Leftrightarrow}Rx=Q^Tb\n\t\\end{align*}\n\t%\n\t\\item $\\textcolor{exampoints}{(2P)}~$ The rank of a real matrix $A\\in\\mathbb{R}^{n\\times m}$ is defined as the number of positive singular values ($=\\text{dim}(\\text{Im}A)~=$ number of linear independent columns).\n\t%\n\t\\item $\\textcolor{exampoints}{(2P)}~\\text{rank}(A)+\\text{dim}(\\text{ker}(A))=n~~(\\text{for}~A\\in\\mathbb{R}^{m\\times n})$\n\t%\n\t\\item \n\t$\\textcolor{exampoints}{(2P)}~A\\in\\mathbb{R}^{m\\times n}$, then the first k principal components are given by\n\t$$\n\tU_k:=\\begin{matrix}\\text{argmin}\\\\\n\t{z\\in\\mathbb{R}^{m\\times k}~\\text{\\tiny{orthogonal}}}\\end{matrix}~\\|A-zz^TA\\|_{F}^2.\n\t$$\t\n\t%\n\t\\item $\\textcolor{exampoints}{(1P)}~~D=~\\text{diag}(d_{ii})$ invertible $\\Leftrightarrow~~d_{ii}\\neq 0~\\forall i$\\\\\n\t$\\textcolor{exampoints}{(1P)}$ Then $D^{-1}=~\\text{diag}(\\frac{1}{d_{ii}})$\n\t%\n\t\\item \n\t$\\textcolor{exampoints}{(1P)}$ Purpose: Compute eigenvalues of a matrix $A\\in\\mathbb{R}^{n\\times n}$\n\t\\begin{align*}\n\t\\textcolor{exampoints}{(1P)}~~A_0&:=A\\\\\n\t\\text{for}~&i=1,\\dots,n\\\\\n\t&Q_iR_i:=A_i\\\\\n\t&A_{i+1}:=R_iQ_i\n\t\\end{align*}\n\t%\n\t\\item $\\rho(M)<1~\\textcolor{exampoints}{(1P)}~~\\Rightarrow~~(x_k)_k$ converges to fixed point $x^*=Mx^*+b~~\\textcolor{exampoints}{(1P)}$\n\t%\n\t\\item $\\textcolor{exampoints}{(1P)}~~Q\\in\\mathbb{R}^{n\\times n}$ orthogonal $:\\Leftrightarrow~~Q^TQ=I$\\\\\n\t$\\textcolor{exampoints}{(1P)}$ \n\tThus the columns of $Q$ are mutually orthonormal.\n\\end{enumerate}\n}\n\\begin{enumerate}\n\t\\item \n\t$\\textcolor{exampoints}{(2P)}~$ The rank of a real matrix $A\\in\\mathbb{R}^{n\\times m}$ is defined as the number of positive singular values ($=\\text{dim}(\\text{Im}A)~=$ number of linear independent columns).\n\t\n\t\\item \n\t$\\textcolor{exampoints}{(2P)}~$ The power iteration is an algorithm to find an eigenvector of a matrix\\\\ $A\\in\\mathbb{R}^{n\\times n}$ corresponding to its largest eigenvalue. \n\tThe drawback of this algorithm is the previous mentioned fact, that the resulting eigenvector is corresponded to the largest eigenvalue.\\\\\n\tThe iterations are computed by $x^{k+1}:=\\frac{Ax^k}{\\|Ax^k\\|}$.\\\\\n\tAs an alternative you can use the Inverse Power iteration.\n\t\n\t\\item \n\t$\\textcolor{exampoints}{(2P)}~A=\\begin{pmatrix}0&1\\\\1&1\\end{pmatrix}$\n\t\n\t\\item $\\textcolor{exampoints}{(2P)}~\\text{rank}(A)+\\text{dim}(\\text{ker}(A))=n~~(\\text{for}~A\\in\\mathbb{R}^{m\\times n})$\n\t\n\t\\item \n\t$\\textcolor{exampoints}{(2P)}~$ No!\n\t\\begin{align*}\n\t&\\lambda_i\\in\\sigma(A^TA),~\\sigma_i:=\\sqrt{\\lambda}=|\\tilde{\\lambda}_i|\\\\\n\t&\\tilde{\\lambda}_i\\in\\sigma(A)~~\\Rightarrow~~\\tilde{\\lambda}_i=\\lambda_i\\in\\sigma(A^TA)=\\sigma(A^2)\n\t\\end{align*}\n\t\\underline{Thus:} They are only equal up to the sign.\n\t\n\t\\item \n\t$\\textcolor{exampoints}{(2P)}~A\\in\\mathbb{R}^{m\\times n}$, then the first k principal components are given by\n\t$$\n\tU_k:=\\begin{matrix}\\text{argmin}\\\\\n\t{z\\in\\mathbb{R}^{m\\times k}~\\text{\\tiny{orthogonal}}}\\end{matrix}~\\|A-zz^TA\\|_{F}^2.\n\t$$\n\t\n\t\\item \n\t$\\textcolor{exampoints}{(2P)}~A\\in\\mathbb{R}^{n\\times n}~ \\text{positive definite}~~\\Rightarrow~~\\forall x\\neq 0:~x^T\\underbrace{Ax}_{\\textcolor{blue}{:=z}}>0$ \n\t\\begin{align*} \n\t0<x^Tz =\\underbrace{\\text{cos}(\\alpha)}_{\\textcolor{blue}{>0}}\\underbrace{\\|x\\|\\|z\\|}_{\\textcolor{blue}{>0}}~~&\\Rightarrow~~\\text{cos}(\\alpha)>0\\\\\n\t&\\Rightarrow~~\\alpha\\in\\left(-\\frac{\\pi}{2},\\frac{\\pi}{2}\\right),~|\\alpha|<90^\\text{o}\n\t\\end{align*}\n\\end{enumerate}"
}